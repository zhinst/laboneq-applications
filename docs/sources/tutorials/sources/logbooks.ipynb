{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7af535",
   "metadata": {},
   "source": [
    "# Recording Experiment Workflow Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39904ef1",
   "metadata": {},
   "source": [
    "While running an experiment workflow one would like to keep a record of what took place -- a kind of digital lab book. LabOne Q provides workflow logbooks for just this task.\n",
    "\n",
    "Each workflow run creates its own logbook. The logbook records the tasks being run and may also be used to store additional data such as device settings, LabOne Q experiments, qubits, and the results of experiments and analyses.\n",
    "\n",
    "Logbooks need to be stored somewhere, and within the Applications Library, this place is called a logbook store.\n",
    "\n",
    "Currently the Applications Library supports two kinds of stores:\n",
    "\n",
    "* `FolderStore`\n",
    "* `LoggingStore`\n",
    "\n",
    "The `FolderStore` writes logbooks to a folder on disk. It is used to keep a permanent record of the experiment workflow.\n",
    "\n",
    "The `LoggingStore` logs what is happening using Python's logging. It provides a quick overview of the steps performed by a workflow.\n",
    "\n",
    "We'll look at each of these in more detail shortly, but first let us set up a quantum platform to run some experiments on so we have something to record."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3c6d8",
   "metadata": {},
   "source": [
    "## Setting up a quantum platform\n",
    "\n",
    "Build your LabOne Q `DeviceSetup`, qubits and `Session` as normal. Here we import a demonstration tunable transmon quantum platform from the library and the amplitude Rabi experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from laboneq.simple import *\n",
    "\n",
    "from laboneq_applications.experiments import amplitude_rabi\n",
    "from laboneq_applications.qpu_types.tunable_transmon import demo_platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93fc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a demonstration QuantumPlatform for a tunable-transmon QPU:\n",
    "qt_platform = demo_platform(n_qubits=6)\n",
    "\n",
    "# The platform contains a setup, which is an ordinary LabOne Q DeviceSetup:\n",
    "setup = qt_platform.setup\n",
    "\n",
    "# And a tunable-transmon QPU:\n",
    "qpu = qt_platform.qpu\n",
    "\n",
    "# Inside the QPU, we have quantum elements, which is a list of six LabOne Q Application\n",
    "# Library TunableTransmonQubit qubits:\n",
    "qubits = qpu.quantum_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fe834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session(setup)\n",
    "session.connect(do_emulation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de77b2f6",
   "metadata": {},
   "source": [
    "## The LoggingStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01fa512",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When you import the `laboneq_applications` library it automatically creates a default `LoggingStore` for you. This logging store is used whenever a workflow is executed and logs information about:\n",
    "\n",
    "* the start and end of workflows\n",
    "* the start and end of tasks\n",
    "* any errors that occur\n",
    "* comments (adhoc messages from tasks, more on these later)\n",
    "* any data files that would be saved if a folder store was in use (more on these later too) \n",
    "\n",
    "These logs don't save anything on disk, but they will allow us to see what events are recorded and what would be saved if we did a have a folder store active."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525e6d9",
   "metadata": {},
   "source": [
    "### An example of logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56c534",
   "metadata": {},
   "source": [
    "Let's run the amplitude Rabi experiment and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f028d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitudes = np.linspace(0.0, 0.9, 10)\n",
    "options = amplitude_rabi.experiment_workflow.options()\n",
    "options.count(10)\n",
    "options.averaging_mode(AveragingMode.CYCLIC)\n",
    "rabi_tb = amplitude_rabi.experiment_workflow(\n",
    "    session,\n",
    "    qpu,\n",
    "    qubits[0],\n",
    "    amplitudes,\n",
    "    options=options,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc87e7",
   "metadata": {},
   "source": [
    "The workflow has not yet been executed, but when you run the next cell, you should see messages like:\n",
    "\n",
    "```\n",
    "──────────────────────────────────────────────────────────────────────────────\n",
    " Workflow 'amplitude_rabi': execution started\n",
    "────────────────────────────────────────────────────────────────────────────── \n",
    "```\n",
    "\n",
    "appear in the logs beneath the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0344003",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rabi_tb.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ca4673",
   "metadata": {},
   "source": [
    "And that's all there is to the basic logging functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ab2a4",
   "metadata": {},
   "source": [
    "### Advanced logging uses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb00a296",
   "metadata": {},
   "source": [
    "If you need to create a logging store of your own you can do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43704384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from laboneq.workflow.logbook import LoggingStore\n",
    "\n",
    "logging_store = LoggingStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb7140",
   "metadata": {},
   "source": [
    "The logging store created above won't be active unless you run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf8f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_store.activate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489914d6",
   "metadata": {},
   "source": [
    "And you deactivate it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e265022",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_store.deactivate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb28acb",
   "metadata": {},
   "source": [
    "You can access the default logging store by importing it from `laboneq.workflow.logbook`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb223305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from laboneq.workflow.logbook import DEFAULT_LOGGING_STORE\n",
    "\n",
    "DEFAULT_LOGGING_STORE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b367e0",
   "metadata": {},
   "source": [
    "You can also inspect all the active logbook stores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from laboneq.workflow.logbook import active_logbook_stores\n",
    "\n",
    "active_logbook_stores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de8267",
   "metadata": {},
   "source": [
    "## The FolderStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a8e62",
   "metadata": {},
   "source": [
    "### Using the folder store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200daa14",
   "metadata": {},
   "source": [
    "The `FolderStore` saves workflow results on disk and is likely the most important logbook store you'll use.\n",
    "\n",
    "You can import it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e351cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from laboneq.workflow.logbook import FolderStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1e03d",
   "metadata": {},
   "source": [
    "To create a folder store you'll need to pick a folder on disk to store logbooks in. Here we select `./experiment_store` as the folder name but you should pick your own.\n",
    "\n",
    "Within the folder, logbooks are organized into folder by date and then stored in sub-folders whose name will start with a timestamp followed by the name of the workflow. For example, the folder layout might look as follows:\n",
    "\n",
    "```\n",
    "experiment_store\n",
    "└── 20240728\n",
    "    └── 20240728T175500-amplitude-rabi\n",
    "        └── ...\n",
    "    └── 20240728T175900-amplitude-rabi\n",
    "        └── ...\n",
    "```\n",
    "\n",
    "Each logbook created by a workflow will have its own sub-folder. If necessary, a unique count will be added at the end to make the sub-folder name unique.\n",
    "\n",
    "Timestamps are in local time.\n",
    "\n",
    "The folder store will need to be activated before workflows will use it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d0425",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_store = FolderStore(\"./experiment_store\")\n",
    "folder_store.activate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06978f7b",
   "metadata": {},
   "source": [
    "Now let's run the amplitude Rabi workflow. As before we'll see the task events being logged. Afterwards we'll explore the folder to see what has been written to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a0909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = rabi_tb.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0442da2",
   "metadata": {},
   "source": [
    "If you no longer wish to automatically store workflow results in the folder store, you can deactivate it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c4f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_store.deactivate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b57fc21",
   "metadata": {},
   "source": [
    "### Exploring what was written to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db36be",
   "metadata": {},
   "source": [
    "Here we will use Python's `pathlib` functionality to explore what has been written to disk, but you can also use whatever ordinary tools you prefer (terminal, file navigator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee4020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2c98a",
   "metadata": {},
   "source": [
    "Remember that above we requested that the folder store use a folder named `experiment_store`. Let's list the logbooks that were created in that folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b63bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_folder = Path(\"experiment_store\")\n",
    "\n",
    "amplitude_rabi_folders = sorted(store_folder.glob(\"*/*-amplitude-rabi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d396fd",
   "metadata": {},
   "source": [
    "Our amplitude Rabi experiment is the most recent one run, so let's look at the files within the most recent folder. Note that the logbook folder names start with a timestamp followed by the name of the workflow, which allows us to easily order them by time and to find the workflow we're looking for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20be055",
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude_rabi_folder = amplitude_rabi_folders[-1]\n",
    "\n",
    "amplitude_rabi_files = sorted(amplitude_rabi_folder.iterdir())\n",
    "amplitude_rabi_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9765d84",
   "metadata": {},
   "source": [
    "Let us look at the file `log.jsonl`. This is the log of what took place. The log is stored in a format called \"JSONL\" which means each line of the log is a simple Python dictionary stored as JSON. Larger objects and certain types of data are stored as separate files in the same folder.\n",
    "\n",
    "Let's open the file and list the logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3018fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_log = amplitude_rabi_folder / \"log.jsonl\"\n",
    "logs = [json.loads(line) for line in experiment_log.read_text().splitlines()]\n",
    "logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e21dc-92f2-4097-8f2c-04844f64e9b9",
   "metadata": {},
   "source": [
    "### Workflow timestamps and names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e56df8d-7a15-4b60-aa9f-c384c8f420f1",
   "metadata": {},
   "source": [
    "The timestamps and workflow names used by the folder store can be accessed via the `execution_info` utility function inside a task, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b2aa4-eb47-47ad-abbb-44650cba1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from laboneq.workflow import (\n",
    "    execution_info,\n",
    "    task,\n",
    "    workflow,\n",
    ")\n",
    "\n",
    "\n",
    "@task\n",
    "def folder_logger_timestamp_and_workflow_name():\n",
    "    info = execution_info()  # Returns a WorkflowExecutionInfoView object\n",
    "    return {\"workflow\": info.workflows[0], \"start_time\": info.start_time}\n",
    "\n",
    "\n",
    "@workflow\n",
    "def timestamp_and_name_workflow():\n",
    "    folder_logger_timestamp_and_workflow_name()\n",
    "\n",
    "\n",
    "wf = timestamp_and_name_workflow()\n",
    "result = wf.run()\n",
    "\n",
    "print(result.tasks[\"folder_logger_timestamp_and_workflow_name\"].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f2c7a-cb8b-4061-880b-3f54b1b84825",
   "metadata": {},
   "source": [
    "The output of `execution_info()` has two attributes:\n",
    "\n",
    "- `.workflows`: a list of the active workflow names, where the outermost workflow is the first element and the innermost workflow is the last element.\n",
    "- `.start_time` is a `datetime.datetime` object specifying the start time of the outermost workflow.\n",
    "\n",
    "The folder store uses this same information when creating its sub-folder names. The `strftime` format `YYYYMMDDTHHMMSS` is used to generate the timestamp strings (i.e., `strftime(\"%Y%m%dT%H%M%S\")`) after conversion from UTC to local time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd8ac1e-0b7e-4699-9c88-a6c45b162110",
   "metadata": {},
   "source": [
    "### Loading back data from a file\n",
    "\n",
    "Currently, the `FolderStore` cannot be used to load back data from a saved file. The intention is that the `FolderStore` saves data in standard formats which can be loaded by other standard tools.\n",
    "\n",
    "For example, to load back a LabOne Q object saved by a `Workflow`, the standard tool would be the LabOne Q serializer's `load` function:\n",
    "\n",
    "```python\n",
    "from laboneq import serializers\n",
    "\n",
    "my_object = serializers.load(path_to_file)\n",
    "```\n",
    "\n",
    "Here, `path_to_file` is the full path to the data file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3621d0c9-2322-4d47-91e0-d895132ffc65",
   "metadata": {},
   "source": [
    "### How the folder store saves data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977432c-9a0f-4f1b-b5f1-b53d9022ae45",
   "metadata": {},
   "source": [
    "When saving task or workflow input or output values, the folder store first checks whether the value is a simple type that can be stored directly in the log file, i.e. `log.jsonl`.\n",
    "\n",
    "Types that are considered simple are:\n",
    "\n",
    "* `None`, `int`, `float`, `bool` -- stored as is.\n",
    "* `complex` -- stored as `{\"real\": obj.real, \"imag\": obj.imag}`.\n",
    "* `str` -- stored as is if `len(obj) <= 1000`, otherwise not stored.\n",
    "* `datetime` -- stored as UTC timestamp.\n",
    "* `date` -- stored as a `str(obj)`.\n",
    "* `list` -- stored if `len(obj) <= 10` and all elements are considered simple, otherwise not stored.\n",
    "* `dict` -- stored if `len(obj) <= 10`, all keys are strings and all values are considered simple, otherwise not stored.\n",
    "* `tuple` -- stored as for `dict` if `obj` is a `namedtuple`, otherwise stored as for `list`.\n",
    "* `laboneq.dsl.session.Session` -- explicitly marked as not to be serialized since the `Session` object is stateful (it holds a connection to the controlled device).\n",
    "\n",
    "If the type is not considered simple, it is saved as a file using the folder store serializer (see below) and a reference to the file is saved in the log. A file reference looks like `{\"filename\": ..., \"description\": ...}` where the filename specifies the path to the file relative to the folder the logbook is being saved in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f52867f-08a0-4061-bb91-54935628fdac",
   "metadata": {},
   "source": [
    "### The folder store serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7e0dca-8f79-4806-924f-438da355170e",
   "metadata": {},
   "source": [
    "If a task or workflow input or output value is not considered simple, or an object is stored directly using `save_artifact` (see [Store data from within tasks](#store-data-from-within-tasks)), then it is saved to disk using the folder store serializer.\n",
    "\n",
    "Task inputs are saved individually using the name of the input parameter. Task outputs are stored as a single object, unless the output is a dictionary, in which case each element is saved individually using the key as its name.\n",
    "\n",
    "The folder store serializer is distinct from the LabOne Q serializer. The LabOne Q serializer saves LabOne Q objects. The folder store serializer saves a much wider range of objects. It stores objects in standard formats that other tools can load. For LabOne Q objects, the standard format is that produced by the LabOne Q serializer, so the folder store uses that for LabOne Q objects.\n",
    "\n",
    "The folder store serializer only saves objects. Objects are intended to be loaded using standard tools. For example, JSON files can be loaded with standard JSON libraries, PNGs can be loaded with image viewers, stored LabOne Q objects can be loaded with LabOne Q.\n",
    "\n",
    "Types that are saved by the folder store and the formats they are stored in are:\n",
    "\n",
    "* `str` -- serialized as a UTF-8 encoded text file (`.txt`)\n",
    "* `bytes` -- serialized as a binary file (`.dat`)\n",
    "* `PIL.Image` -- saved using `PIL.Image.save` (default format is `PNG`)\n",
    "* `matplotlib.figure.Figure` -- saved using `matplotlib.figure.Figure.savefig` (default format is `PNG`)\n",
    "* `numpy.ndarray` -- saved using `numpy.save` (`.npy`)\n",
    "* `lmfit.model.ModelResult` -- saved as JSON using the dictionary from `lmfit.model.ModelResult.summary` (`.json`)\n",
    "* `uncertainties.core.Variable` -- the `value`, `std_dev` and `tag` attributes are saved as a JSON dictionary (`.json`)\n",
    "* `uncertainties.core.AffineScalarFunc` -- the `value` and `std_dev` attributes are saved as a JSON dictionary (`.json`)\n",
    "* `sklearn.base.BaseEstimator` -- the estimator parameters are saved as a JSON dictionary (`.json`)\n",
    "* LabOne Q objects -- see list of supported LabOne Q objects below, saved using the LabOne Q serializer (`.json`).\n",
    "* `list` -- saved as `numpy.ndarray`, except for lists of `QuantumElement` or lists of `QuantumParameters`, which are saved using the LabOne Q serializer\n",
    "* `tuple` -- saved as a JSON list, except for tuples of `QuantumElement` or tuples of `QuantumParameters`, which are saved using the LabOne Q serializer\n",
    "* `dict` -- saved as a JSON dict, except for dictionaries of `QuantumElement` or dictionaries of `QuantumParameters`, which are saved using the LabOne Q serializer\n",
    "\n",
    "The types of LabOne Q objects that are supported by the folder store are:\n",
    "\n",
    "* `CompiledExperiment`\n",
    "* `DeviceSetup`\n",
    "* `Experiment`\n",
    "* `QPU`\n",
    "* `QuantumParameters` (and lists, tuples, and dictionaries of these)\n",
    "* `QuantumElement` (and lists. tuples, and dictionaries of these)\n",
    "* `Results`\n",
    "* `TaskOptions`\n",
    "* `WorkflowOptions`\n",
    "\n",
    "When the folder store saves objects as JSON, it uses its own extended JSON serializer that supports the following types:\n",
    "\n",
    "* `None`, `int`, `float`, `bool`, `str`, `numpy.integer` -- stored directly as JSON equivalent\n",
    "* `complex` -- stored as `{\"real\": c.real, \"imag\": c.imag}`\n",
    "* `dict` -- supported if keys are strings and values are other supported objects\n",
    "* `list` -- of other supported objects\n",
    "* `tuple` -- of other supported objects\n",
    "* `numpy.ndarray` -- most dtypes are stored directly as lists of appropriate types; complex arrays are in custom structure (see next entry); object arrays are not supported\n",
    "* complex `numpy.ndarray`:  stored as `{\"description\": ..., \"data\": ...}` where `data` is a list of `[real(v0), imag(v0), real(v1), imag(v1), ...]` corresponding to numpy's `.view(dtype=float_dtype)`, and `description` is a string describing the format\n",
    "* `lmfit.model.ModelResult` -- stored as the dictionary from `lmfit.model.ModelResult.summary`\n",
    "* `uncertainties.core.Variable` -- the `value`, `std_dev` and `tag` attributes are stored as a dictionary\n",
    "* `uncertainties.core.AffineScalarFunc` -- the `value` and `std_dev` attributes are saved as a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bac801-d91f-421b-a579-9874a12db7be",
   "metadata": {},
   "source": [
    "### Raising exceptions when inputs and outputs cannot be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3977838-a7b6-4097-a359-e5314c4e332c",
   "metadata": {},
   "source": [
    "By default a folder store logs a warning if a task input or output cannot be saved, but this behavior can be changed by setting the `save_mode` option when creating the folder store or by calling the `.save_mode` method.\n",
    "\n",
    "The supported save modes are:\n",
    "\n",
    "* WARN: A warning is logged when an input or output cannot be saved (the default mode).\n",
    "* RAISE: An exception is raised when an input or output cannot be saved.\n",
    "* SKIP: Task inputs and outputs are not saved.\n",
    "\n",
    "Let's create a new folder store that doesn't save task inputs and outputs, and examine the set save mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd3c482-ddca-4b1e-a1e3-598fc12ded64",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_store_skip = FolderStore(\"./experiment_store\", save_mode=\"skip\")\n",
    "folder_store_skip.save_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c5126-9c00-44a5-8c90-744f15477cf5",
   "metadata": {},
   "source": [
    "We can also modify the save mode of an existing folder store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a7c26-c1f9-425a-b88a-0b294028445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_store_skip.save_mode(\"raise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7907ce",
   "metadata": {},
   "source": [
    "In the remaining sections, we'll look at how to write adhoc comments into the logs and how to save data files to disk.\n",
    "\n",
    "The timestamp of the start time of the workflow execution and the name(s) of the currently executed workflow(s) (if the task was executed from a workflow) can be obtained from within a task. If the task was not called from within a workflow execution context, the timestamp will be `None` and the workflow names will be an empty list. Timestamp and the first of the workflow names are also part of the folder path in case a folder logger is used. Here is an example of a task which reads the outermost workflow's name and the timestamp:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c562045",
   "metadata": {},
   "source": [
    "## Logging comments from within tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11851319",
   "metadata": {},
   "source": [
    "Logbooks allow tasks to add their own messages to the logbook as comments.\n",
    "\n",
    "This is done by calling the `comment(...)` function within a task.\n",
    "\n",
    "We'll work through an example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa50201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from laboneq.workflow import comment, task, workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d96e4",
   "metadata": {},
   "source": [
    "Let's write a small workflow and a tiny task that just writes a comment to the logbook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ae967",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def log_a_comment(msg):\n",
    "    comment(msg)\n",
    "\n",
    "\n",
    "@workflow\n",
    "def demo_comments():\n",
    "    log_a_comment(\"Activating multi-state discrimination! <sirens blare>\")\n",
    "    log_a_comment(\"Analysis successful! <cheers>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6da687",
   "metadata": {},
   "source": [
    "Now when we run the workflow we'll see the comments appear in the logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedb8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = demo_comments()\n",
    "result = wf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fb085b",
   "metadata": {},
   "source": [
    "Above you should see the two comments. They look like this:\n",
    "```\n",
    "Comment: Activating multi-state discrimination! <sirens blare>\n",
    "...\n",
    "Comment: Analysis successful! <cheers>\n",
    "```\n",
    "\n",
    "In addition to `comment(...)`, the logbook supports a function `log(level: int, message: str, *args: object)` which logs a message at the specified logging level similar to Python's `logging` module. This additional function is useful for logging messages that are not regular user comments, but allow tasks to give feedback about issues which are still important to record."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cae763",
   "metadata": {},
   "source": [
    "## Store data from within tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481be5b6",
   "metadata": {},
   "source": [
    "Logbooks also allow files to be saved to disk using the function `save_artifact`.\n",
    "\n",
    "Here we will create a figure with matplotlib and save it to disk. The folder store will automatically save it as a PNG.\n",
    "\n",
    "The kinds of objects the folder store serializes are described above in [The folder store serializer](#the-folder-store-serializer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ab3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from laboneq.workflow import save_artifact\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06170915",
   "metadata": {},
   "source": [
    "Let's write a small workflow that plots the sine function and saves the plot using `save_artifact`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa631fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def sine_plot():\n",
    "    fig = plt.figure()\n",
    "    plt.title(\"A sine wave\")\n",
    "    x = np.linspace(0, 2 * np.pi, 100)\n",
    "    y = np.sin(x)\n",
    "    plt.plot(x, y)\n",
    "\n",
    "    save_artifact(\"Sine Plot\", fig)\n",
    "\n",
    "\n",
    "@workflow\n",
    "def demo_saving():\n",
    "    sine_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b0218",
   "metadata": {},
   "source": [
    "Since we deactivated the folder store, let's activate it again now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_store.activate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bf2379",
   "metadata": {},
   "source": [
    "And run our workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fb2070",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = demo_saving()\n",
    "result = wf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205ec34",
   "metadata": {},
   "source": [
    "You can see in the logs that an artifact was created:\n",
    "```\n",
    "Artifact: 'Sine Plot' of type 'Figure' logged\n",
    "```\n",
    "Now let's load the image from disk.\n",
    "\n",
    "First we need to find the logbook folder created for our workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c5ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_saving_folders = sorted(store_folder.glob(\"*/*-demo-saving\"))\n",
    "demo_saving_folder = demo_saving_folders[-1]\n",
    "demo_saving_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c533fe4",
   "metadata": {},
   "source": [
    "And let's list its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6db07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(demo_saving_folder.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d36cd",
   "metadata": {},
   "source": [
    "And finally let's load the saved image using PIL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(demo_saving_folder / \"Sine Plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c7e9a",
   "metadata": {},
   "source": [
    "Saving an object also generates an entry in the folder store log.\n",
    "\n",
    "We can view it by opening the log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a694b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_log = demo_saving_folder / \"log.jsonl\"\n",
    "logs = [json.loads(line) for line in experiment_log.read_text().splitlines()]\n",
    "logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ad6a3",
   "metadata": {},
   "source": [
    "As you can see above the log records the name (`artifact_name`) and type (`artifact_type`) of the object saved, and the name of the file it was written to (`artifact_files`)\n",
    "\n",
    "Saving an artifact might potentially write multiple files to disk.\n",
    "\n",
    "The `artifact_metadata` contains additional user supplied information about the object saved, while `artifact_options` provide initial information on how to save the object. For example, we could have elected to save the figure in another file format. We'll see how to use both next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9acaf61",
   "metadata": {},
   "source": [
    "### Specifying metadata and options when saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa53a3d",
   "metadata": {},
   "source": [
    "Let's again make a small workflow that saves a plot, but this time we'll add some options and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8012f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def sine_plot_with_options():\n",
    "    fig = plt.figure()\n",
    "    plt.title(\"A sine wave\")\n",
    "    x = np.linspace(0, 2 * np.pi, 100)\n",
    "    y = np.sin(x)\n",
    "    plt.plot(x, y)\n",
    "    [ax] = fig.get_axes()\n",
    "\n",
    "    save_artifact(\n",
    "        \"Sine Plot\",\n",
    "        fig,\n",
    "        metadata={\n",
    "            \"title\": ax.get_title(),\n",
    "        },\n",
    "        options={\n",
    "            \"format\": \"jpg\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@workflow\n",
    "def demo_saving_with_options():\n",
    "    sine_plot_with_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f115c0",
   "metadata": {},
   "source": [
    "And run the workflow to save the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995bad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = demo_saving_with_options()\n",
    "result = wf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c470aa62",
   "metadata": {},
   "source": [
    "Again we open the workflow folder and load the saved image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_saving_with_options_folders = sorted(\n",
    "    store_folder.glob(\"*/*-demo-saving-with-options\")\n",
    ")\n",
    "demo_saving_with_options_folder = demo_saving_with_options_folders[-1]\n",
    "demo_saving_with_options_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09292cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(demo_saving_with_options_folder.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954c37e",
   "metadata": {},
   "source": [
    "Now when we load the image it is very slightly blurry, because it was saved as a JPEG which uses lossy compression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(demo_saving_with_options_folder / \"Sine Plot.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a9ef9",
   "metadata": {},
   "source": [
    "And if we view the logs we can see that the title was recorded in the `artifact_metadata`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_log = demo_saving_with_options_folder / \"log.jsonl\"\n",
    "logs = [json.loads(line) for line in experiment_log.read_text().splitlines()]\n",
    "logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223d5c18",
   "metadata": {},
   "source": [
    "The supported options for saving artifacts depend on the type of artifact. For our matplotlib figure example, the options are forwarded to `matplotlib.pyplot.savefig` and are documented in the [Matplotlib documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html), with the following changes to the default values:\n",
    "\n",
    "* `format` is set to \"png\" by default\n",
    "* `bbox_inches` is set to \"tight\" by default\n",
    "\n",
    "In the same way, the options for a `PIL.Image.Image` are forwarded to `PIL.Image.Image.save` and are documented in the [Pillow documentation](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.save) with the format defaulting to \"PNG\". For a `numpy.ndarray` the options are forwarded to `numpy.save` and are documented in the [Numpy documentation](https://numpy.org/doc/stable/reference/generated/numpy.save.html) with `allow_pickle` set to `False` by default.\n",
    "\n",
    "We're done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
