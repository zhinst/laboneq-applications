{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd2ddc8",
   "metadata": {},
   "source": [
    "# Build a taskbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f977a",
   "metadata": {},
   "source": [
    "A taskbook is a collection of tasks and their records.\n",
    "\n",
    "Whenever a task is executed within a taskbook, its'\n",
    "arguments, result and other relevant information are recorded.\n",
    "\n",
    "The main components of an taskbook are the following:\n",
    "\n",
    "* **Task**:\n",
    "\n",
    "    `@task`: A decorator to mark a function as a task\n",
    "\n",
    "* **Taskbook**:\n",
    "\n",
    "    `@taskbook`: A decorator to convert a function into a taskbook\n",
    "\n",
    "    `TaskBook`: Result of an executed function decorated with `@taskbook`\n",
    "\n",
    "\n",
    "## Building a simple taskbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d02cf",
   "metadata": {},
   "source": [
    "### Import relevant objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef3902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from laboneq_applications.workflow import task, taskbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff21ff4b",
   "metadata": {},
   "source": [
    "### Define the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4344b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def measure() -> int:\n",
    "    return 100\n",
    "\n",
    "\n",
    "@task\n",
    "def analyze(measurement_result: int, threshold: int) -> bool:\n",
    "    return measurement_result < threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a8e595",
   "metadata": {},
   "source": [
    "### Define the taskbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f905d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@taskbook\n",
    "def experiment(threshold: int):\n",
    "    measurement = measure()\n",
    "    analysis_result = analyze(measurement, threshold)\n",
    "    if analysis_result:\n",
    "        return \"PASS\"\n",
    "    return \"FAIL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a37e93",
   "metadata": {},
   "source": [
    "### Run the taskbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f1f9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "book = experiment(threshold=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c72f4",
   "metadata": {},
   "source": [
    "### Inspecting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2a665e",
   "metadata": {},
   "source": [
    "#### Inspecting the `TaskBook`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b5661b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Taskbook\n",
       "Tasks: Task(measure), Task(analyze)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ae7e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PASS'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264fce95",
   "metadata": {},
   "source": [
    "#### Inspecting the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33f7de03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task(measure), Task(analyze)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book.tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbc400",
   "metadata": {},
   "source": [
    "There are several ways to get the individual tasks from the `TaskBook`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a470f9",
   "metadata": {},
   "source": [
    "Single task with an index or name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f3f77ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Task(analyze), Task(analyze))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book.tasks[1], book.tasks[\"analyze\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13e08c8",
   "metadata": {},
   "source": [
    "Specific task lookup with indexing\n",
    "\n",
    "The first argument is the name of the task and the second is an integer or a\n",
    "`slice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4b148e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Task(analyze)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book.tasks[\"analyze\", :]  # All tasks named 'analyze'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdc017",
   "metadata": {},
   "source": [
    "### Rerunning a task\n",
    "\n",
    "A task can be rerun from the `TaskBook` \n",
    "\n",
    "This can be useful when one wants to modify only specific arguments of the task,\n",
    "e.g a plotting arguments.\n",
    "\n",
    "In this example we will only lower the `threshold` of the `analyze` task \n",
    "and since we did not supply the argument `measurement_result`, the original value\n",
    "is used and our `analyze` should now fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "301880e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book.tasks[\"analyze\"].rerun(threshold=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b5a2f8",
   "metadata": {},
   "source": [
    "After rerunning the task, we can also see that the `TaskBook` is updated with\n",
    "the rerun results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cb6c7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Task(analyze), Task(analyze)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book.tasks[\"analyze\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd69666a-d6c6-4c52-936d-b37802ad56d7",
   "metadata": {},
   "source": [
    "### Run until a task\n",
    "\n",
    "A `TaskBook` can be run only up to (and including) one of its tasks.\n",
    "\n",
    "This can be useful if one wants to run only the first few tasks and inspect the result. For example, for an experiment `TaskBook`, one often wants to check if the experiment sequence compiles successfully before running the experiment.\n",
    "\n",
    "In this example, we will exclude the `analyze` task to first inspect if the `measure` task was successful. To do this, we need to allow `experiment` to accept an options dictionary and then pass the taskbook option `run_until`. We can also use options classes as shown in the [`options.ipynb`](options.ipynb) guide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee67a0b-066f-4b14-8f91-65da07b6ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@taskbook\n",
    "def experiment(threshold: int, options: dict):\n",
    "    measurement = measure()\n",
    "    analysis_result = analyze(measurement, threshold)\n",
    "    if analysis_result:\n",
    "        return \"PASS\"\n",
    "    return \"FAIL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e51caac2-d929-4de1-8ded-af6c99514864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options = {\"taskbook.run_until\": \"measure\"}\n",
    "book = experiment(threshold=101, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e6e45ee-2696-4fa0-8d31-4bc4c40fb5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Taskbook\n",
       "Tasks: Task(measure), Task(analyze)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f2c08-c87c-4f0a-a242-d7d24c139cf7",
   "metadata": {},
   "source": [
    "### Inspect a `TaskBook` that has failed\n",
    "\n",
    "In case there is an error during the execution of a `TaskBook`, we can still inspect the tasks that have run up to the task that triggered the error using `recover()`. Note that `recover()` stores only one execution result and can only be called once; a second call to `recover()` raises an exception.\n",
    "\n",
    "For experiment `TaskBooks`, this is useful for debugging a failed compilation task by inspecting the experiment sequence produced by the previous task. \n",
    "\n",
    "In this example, we will add an assertion error to the `analyze` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4637138-4f85-4fbc-bbd0-f1eff632c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def measure() -> int:\n",
    "    return 100\n",
    "\n",
    "\n",
    "@task\n",
    "def analyze(measurement_result: int, threshold: int) -> bool:\n",
    "    # let's add an error in this task\n",
    "    if not (measurement_result >= 100 and threshold >= 100):\n",
    "        raise RuntimeError(\"Something went wrong.\")\n",
    "    return measurement_result < threshold\n",
    "\n",
    "\n",
    "@taskbook\n",
    "def experiment(threshold: int):\n",
    "    measurement = measure()\n",
    "    analysis_result = analyze(measurement, threshold)\n",
    "    if analysis_result:\n",
    "        return \"PASS\"\n",
    "    return \"FAIL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c65e802-fe77-4d4d-9a3c-2160d321399f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Something went wrong.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m book \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m99\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\working_folder\\laboneq-applications\\src\\laboneq_applications\\workflow\\taskbook.py:515\u001b[0m, in \u001b[0;36mtaskbook_.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _TaskBookExecutor(\n\u001b[0;32m    511\u001b[0m     book,\n\u001b[0;32m    512\u001b[0m     options\u001b[38;5;241m=\u001b[39mopt,\n\u001b[0;32m    513\u001b[0m ):\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         book\u001b[38;5;241m.\u001b[39m_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m         _results\u001b[38;5;241m.\u001b[39mresults[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func_full_path()] \u001b[38;5;241m=\u001b[39m book\n",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m, in \u001b[0;36mexperiment\u001b[1;34m(threshold)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;129m@taskbook\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexperiment\u001b[39m(threshold: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m     16\u001b[0m     measurement \u001b[38;5;241m=\u001b[39m measure()\n\u001b[1;32m---> 17\u001b[0m     analysis_result \u001b[38;5;241m=\u001b[39m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeasurement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m analysis_result:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPASS\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\working_folder\\laboneq-applications\\src\\laboneq_applications\\workflow\\task.py:100\u001b[0m, in \u001b[0;36mtask_.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: T\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: T\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m B:  \u001b[38;5;66;03m# noqa: D102\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\working_folder\\laboneq-applications\\src\\laboneq_applications\\workflow\\task.py:58\u001b[0m, in \u001b[0;36m_BaseTask.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\working_folder\\laboneq-applications\\src\\laboneq_applications\\workflow\\taskbook.py:423\u001b[0m, in \u001b[0;36m_TaskBookExecutor.execute_task\u001b[1;34m(self, task, *args, **kwargs)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not require options.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 423\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m entry \u001b[38;5;241m=\u001b[39m Task(\n\u001b[0;32m    425\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    426\u001b[0m     output\u001b[38;5;241m=\u001b[39mr,\n\u001b[0;32m    427\u001b[0m     parameters\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mcreate_argument_map(task\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[0;32m    428\u001b[0m )\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtaskbook\u001b[38;5;241m.\u001b[39madd_entry(entry)\n",
      "File \u001b[1;32mC:\\working_folder\\laboneq-applications\\src\\laboneq_applications\\workflow\\task.py:103\u001b[0m, in \u001b[0;36mtask_._run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: T\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: T\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m B:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36manalyze\u001b[1;34m(measurement_result, threshold)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;129m@task\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze\u001b[39m(measurement_result: \u001b[38;5;28mint\u001b[39m, threshold: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# let's add an error in this task\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (measurement_result \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSomething went wrong.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m measurement_result \u001b[38;5;241m<\u001b[39m threshold\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Something went wrong."
     ]
    }
   ],
   "source": [
    "book = experiment(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e22e8a5-b1b5-4e44-a194-0aa341fdd754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Taskbook\n",
       "Tasks: Task(measure)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recovered_book = experiment.recover()\n",
    "recovered_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2991be74-477d-4f73-8f6d-d0cb0ceaf3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the measure task returns a result that is >= 100\n",
    "recovered_book.tasks[\"measure\"].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b512780-83fe-4e8f-8e0e-a120ede3acee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'threshold': 99}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the value of the threshold passed to the taskbook\n",
    "recovered_book.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb4c999-64ac-4b07-9235-da5f7d456531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we know we have to increase the value of the threshold\n",
    "book = experiment(101)\n",
    "book.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87625322-39c2-470c-9668-785f9bc66f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
